{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3039,
     "status": "ok",
     "timestamp": 1548797869231,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "PMGwZ7aFJL8Y",
    "outputId": "6dbaecf5-e875-4bd5-cf95-2ef1d14d906b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.1.0\n",
      "Commit 80516ca202 (2019-01-21 21:24 UTC)\n",
      "Platform Info:\n",
      "  OS: Linux (x86_64-pc-linux-gnu)\n",
      "  CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.1 (ORCJIT, broadwell)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlR1bqvJdmXN"
   },
   "outputs": [],
   "source": [
    "using DataStructures\n",
    "using BSON: @save, @load\n",
    "import Reinforce\n",
    "using Reinforce: CartPoleV0, actions, reset!, finished, step!\n",
    "using Flux, CuArrays, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26095,
     "status": "ok",
     "timestamp": 1548797896347,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "P8y8ZF2leZc3",
    "outputId": "522d4b55-51fe-4716-bf4b-a59d0ec5e712"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"100\""
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr() # gr is faster than pyplot\n",
    "ENV[\"GKSwstype\"] = \"100\" # plotting in headless environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1296,
     "status": "ok",
     "timestamp": 1548799281229,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "kO5PfFbM4SlA",
    "outputId": "14e7b8f3-23cf-44d0-901e-5cdcfd08deac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_model (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------Initialize game environment----------------#\n",
    "env = CartPoleV0()\n",
    "\n",
    "\n",
    "#-------------------------Parameters-----------------------#\n",
    "EPISODES = 500\n",
    "STATE_SIZE = length(env.state)\n",
    "ACTION_SIZE = length(actions(env, env.state))\n",
    "REPLAY_MEMORY = 10000 # buffer size\n",
    "MAX_STEPS = 300 # maximum timesteps per episode\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "γ = 0.99                # discount rate\n",
    "η = 0.0001              # learning rate\n",
    "\n",
    "ϵ = 0.9                 # exploration rate\n",
    "ϵ_min = 0.01            # exploration minimum\n",
    "ϵ_decay = 0.995         # exploration decay\n",
    "\n",
    "memory = CircularBuffer{Any}(REPLAY_MEMORY)\n",
    "\n",
    "\n",
    "#-----------------------Model Architecture------------------------#\n",
    "model = Chain(Dense(STATE_SIZE, 24, relu),\n",
    "              Dense(24, 48, relu), \n",
    "              Dense(48, ACTION_SIZE)) |> gpu\n",
    "\n",
    "loss(x, y) = Flux.mse(model(x), y)\n",
    "opt = ADAM(η)\n",
    "\n",
    "fit_model(dataset) = Flux.train!(loss, params(model), dataset, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1548799285527,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "GO-ZLvfDe9K0",
    "outputId": "e79d1b79-2bb8-4ee6-d4c8-2e065d8dd6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "replay"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Save sample (s, a, r, s′) to replay memory\"\"\"\n",
    "function remember(state, action, reward, next_state, done)\n",
    "    push!(memory, (state, action, reward, next_state, done))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"Get action from model using epsilon-greedy policy\"\"\"\n",
    "function act(state, ϵ)\n",
    "    rand() <= ϵ && return rand(1:ACTION_SIZE)\n",
    "    q_values = model(state |> gpu).data # act values\n",
    "    return argmax(q_values)  # returns action (idx of q value)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"Sample from replay memory, train model, update exploration\"\"\"\n",
    "function replay()\n",
    "    length(memory) < BATCH_SIZE && return nothing\n",
    "    \n",
    "    batch_size = min(BATCH_SIZE, length(memory))\n",
    "    minibatch = sample(memory, batch_size, replace=false)\n",
    "    \n",
    "    sb, ab, rb, s′b, db = collect.(zip(minibatch...))\n",
    "    sb = hcat(sb...) |> gpu\n",
    "    s′b = hcat(s′b...) |> gpu\n",
    "    \n",
    "    qb_target = model(sb).data\n",
    "    qb_learned = maximum(model(s′b).data, dims=1)\n",
    "    qb_learned = ifelse.(db, rb, rb .+ γ .* cpu(qb_learned))\n",
    "    setindex!.(Ref(qb_target), qb_learned, ab) # (1, batch_size)\n",
    "    \n",
    "    dataset = [(sb, qb_target)] # [(input, target)]\n",
    "    fit_model(dataset)\n",
    "    \n",
    "    global ϵ\n",
    "    ϵ > ϵ_min && (ϵ *= ϵ_decay)\n",
    "    \n",
    "    GC.gc(); # CuArrays.clearpool()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1484703,
     "status": "ok",
     "timestamp": 1548800820446,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "q8R4h6_deprJ",
    "outputId": "7db4ab59-b7c6-4962-b24b-6f319fcba756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/500 | Score: 12.0 | ϵ: 0.9\n",
      "Episode: 3/500 | Score: 15.0 | ϵ: 0.9\n",
      "Episode: 4/500 | Score: 17.0 | ϵ: 0.8955\n",
      "Episode: 6/500 | Score: 21.0 | ϵ: 0.8865673875\n",
      "Episode: 15/500 | Score: 32.0 | ϵ: 0.8474605262229382\n",
      "Episode: 16/500 | Score: 38.0 | ϵ: 0.8432232235918236\n",
      "#-- Avg Test Score 1 : 17.7 --#\n",
      "Episode: 52/500 | Score: 65.0 | ϵ: 0.7040013079012841\n",
      "Episode: 63/500 | Score: 120.0 | ϵ: 0.6662348619270341\n",
      "Episode: 96/500 | Score: 129.0 | ϵ: 0.564662593848428\n",
      "#-- Avg Test Score 2 : 51.4 --#\n",
      "Episode: 110/500 | Score: 172.0 | ϵ: 0.5263954772927323\n",
      "#-- Avg Test Score 3 : 48.7 --#\n",
      "#-- Avg Test Score 4 : 30.9 --#\n",
      "#-- Avg Test Score 5 : 23.0 --#\n",
      "#-- Avg Test Score 6 : 14.6 --#\n",
      "#-- Avg Test Score 7 : 10.1 --#\n",
      "#-- Avg Test Score 8 : 8.0 --#\n",
      "#-- Avg Test Score 9 : 7.9 --#\n",
      "#-- Avg Test Score 10 : 7.6 --#\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Training & Testing---------------------------#\n",
    "best_score = 0.0\n",
    "test_every, TEST = Integer(EPISODES/10), 10\n",
    "\n",
    "for e=1:EPISODES\n",
    "    reset!(env)\n",
    "    state = env.state\n",
    "    score = 0\n",
    "    \n",
    "    envs = []\n",
    "    for step=1:MAX_STEPS\n",
    "        push!(envs, deepcopy(env))\n",
    "        \n",
    "        action = act(state, ϵ) # predict action\n",
    "        reward, next_state = step!(env, state, action)\n",
    "        done = finished(env, next_state) # check if game is finished\n",
    "        reward = !done ? reward : -1 # penalty of -1 if game is over\n",
    "        score += reward\n",
    "        \n",
    "        remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        done && break\n",
    "    end\n",
    "    \n",
    "    stats = \"Episode: $e/$EPISODES | Score: $score | ϵ: $ϵ\"\n",
    "    # Episode X finished after Y timesteps with Z total reward\n",
    "    \n",
    "    if best_score < score\n",
    "        best_score = score\n",
    "        println(stats); flush(stdout)\n",
    "        @save \"models/notebook3/model-$e-$score.bson\" model\n",
    "        anim = @animate for env in envs\n",
    "            plot(env)\n",
    "        end\n",
    "        mp4(anim, \"models/dqn/env-$e-$score.mp4\", fps=20, show_msg=false)\n",
    "    else\n",
    "        print(stats); flush(stdout); print(\"\\r\")\n",
    "    end\n",
    "    \n",
    "    replay() # replay and learn from the episode\n",
    "    \n",
    "    if e % test_every == 0\n",
    "        score = 0\n",
    "        for i=1:TEST\n",
    "            reset!(env)    \n",
    "            state = env.state\n",
    "            \n",
    "            for step=1:MAX_STEPS\n",
    "                action = act(state, ϵ_min)\n",
    "                reward, state = step!(env, state, action)\n",
    "                done = finished(env, state) # check if game is finished\n",
    "                reward = !done ? reward : -1 # penalty of -1 if game is over\n",
    "                score += reward\n",
    "\n",
    "                done && break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        score /= TEST\n",
    "        println(\"#-- Avg Test Score $(Integer(e/test_every)) : $score --#\")\n",
    "        score >= 200 && break\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Done!\")\n",
    "# takes about ~ 30 mins to train & test for 500 episodes and 32 batch size: giving best score of 172"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03 - Flux DQN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.1",
   "name": "julia-1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
