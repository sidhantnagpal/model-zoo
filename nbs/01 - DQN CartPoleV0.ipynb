{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 - Flux DQN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"julia-1.1","display_name":"Julia 1.1"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ExSr_6xk3p2Z","colab_type":"code","outputId":"661145ef-df63-4395-8961-88da840b8b83","executionInfo":{"status":"ok","timestamp":1548797863342,"user_tz":-330,"elapsed":3235,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["pwd()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"/content\""]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"KnPjAMwZ30wK","colab_type":"code","outputId":"58b168fb-4dd2-4465-e5ec-f8bac2de103f","executionInfo":{"status":"ok","timestamp":1548797865977,"user_tz":-330,"elapsed":1717,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["cd(\"/content/drive/My Drive/julia\")\n","pwd()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"/content/drive/My Drive/julia\""]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"PMGwZ7aFJL8Y","colab_type":"code","outputId":"6dbaecf5-e875-4bd5-cf95-2ef1d14d906b","executionInfo":{"status":"ok","timestamp":1548797869231,"user_tz":-330,"elapsed":3039,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["versioninfo()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Julia Version 1.1.0\n","Commit 80516ca202 (2019-01-21 21:24 UTC)\n","Platform Info:\n","  OS: Linux (x86_64-pc-linux-gnu)\n","  CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n","  WORD_SIZE: 64\n","  LIBM: libopenlibm\n","  LLVM: libLLVM-6.0.1 (ORCJIT, broadwell)\n"],"name":"stdout"}]},{"metadata":{"id":"EWe2cfjkdqiL","colab_type":"code","colab":{}},"cell_type":"code","source":["# import Pkg\n","# Pkg.add(\"DataStructures\")\n","# Pkg.add([\"Reinforce\", \"StatsBase\", \"Plots\"])\n","# Pkg.add(\"BSON\") # see https://github.com/FluxML/Flux.jl/blob/master/docs/src/saving.md"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XlR1bqvJdmXN","colab_type":"code","colab":{}},"cell_type":"code","source":["using DataStructures\n","using BSON: @save, @load\n","import Reinforce\n","using Reinforce: CartPoleV0, actions, reset!, finished, step!\n","using Flux, CuArrays, StatsBase, Plots"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P8y8ZF2leZc3","colab_type":"code","outputId":"522d4b55-51fe-4716-bf4b-a59d0ec5e712","executionInfo":{"status":"ok","timestamp":1548797896347,"user_tz":-330,"elapsed":26095,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["gr() # gr is faster than pyplot\n","ENV[\"GKSwstype\"] = \"100\" # plotting in headless environment"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"100\""]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"kO5PfFbM4SlA","colab_type":"code","outputId":"14e7b8f3-23cf-44d0-901e-5cdcfd08deac","executionInfo":{"status":"ok","timestamp":1548799281229,"user_tz":-330,"elapsed":1296,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#---------------Initialize game environment----------------#\n","env = CartPoleV0()\n","\n","\n","#-------------------------Parameters-----------------------#\n","EPISODES = 500\n","STATE_SIZE = length(env.state)\n","ACTION_SIZE = length(actions(env, env.state))\n","REPLAY_MEMORY = 10000 # buffer size\n","MAX_STEPS = 300 # maximum timesteps per episode\n","\n","BATCH_SIZE = 32\n","\n","γ = 0.99                # discount rate\n","η = 0.0001              # learning rate\n","\n","ϵ = 0.9                 # exploration rate\n","ϵ_min = 0.01            # exploration minimum\n","ϵ_decay = 0.995         # exploration decay\n","\n","memory = CircularBuffer{Any}(REPLAY_MEMORY)\n","\n","\n","#-----------------------Model Architecture------------------------#\n","model = Chain(Dense(STATE_SIZE, 24, relu),\n","              Dense(24, 48, relu), \n","              Dense(48, ACTION_SIZE)) |> gpu\n","\n","loss(x, y) = Flux.mse(model(x), y)\n","opt = ADAM(η)\n","\n","fit_model(dataset) = Flux.train!(loss, params(model), dataset, opt)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fit_model (generic function with 1 method)"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"GO-ZLvfDe9K0","colab_type":"code","outputId":"e79d1b79-2bb8-4ee6-d4c8-2e065d8dd6ba","executionInfo":{"status":"ok","timestamp":1548799285527,"user_tz":-330,"elapsed":1728,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["\"\"\"Save sample (s, a, r, s′) to replay memory\"\"\"\n","function remember(state, action, reward, next_state, done)\n","    push!(memory, (state, action, reward, next_state, done))\n","end\n","\n","\n","\"\"\"Get action from model using epsilon-greedy policy\"\"\"\n","function act(state, ϵ)\n","    rand() <= ϵ && return rand(1:ACTION_SIZE)\n","    q_values = model(state |> gpu).data # act values\n","    return argmax(q_values)  # returns action (idx of q value)\n","end\n","\n","\n","\"\"\"Sample from replay memory, train model, update exploration\"\"\"\n","function replay()\n","    length(memory) < BATCH_SIZE && return nothing\n","    \n","    batch_size = min(BATCH_SIZE, length(memory))\n","    minibatch = sample(memory, batch_size, replace=false)\n","    \n","    sb, ab, rb, s′b, db = collect.(zip(minibatch...))\n","    sb = hcat(sb...) |> gpu\n","    s′b = hcat(s′b...) |> gpu\n","    \n","    qb_target = model(sb).data\n","    qb_learned = maximum(model(s′b).data, dims=1)\n","    qb_learned = ifelse.(db, rb, rb .+ γ .* cpu(qb_learned))\n","    setindex!.(Ref(qb_target), qb_learned, ab) # (1, batch_size)\n","    \n","    dataset = [(sb, qb_target)] # [(input, target)]\n","    fit_model(dataset)\n","    \n","    global ϵ\n","    ϵ > ϵ_min && (ϵ *= ϵ_decay)\n","    \n","    GC.gc(); # CuArrays.clearpool()\n","end"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["replay"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"q8R4h6_deprJ","colab_type":"code","outputId":"7db4ab59-b7c6-4962-b24b-6f319fcba756","executionInfo":{"status":"ok","timestamp":1548800820446,"user_tz":-330,"elapsed":1484703,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"cell_type":"code","source":["#----------------------------Training & Testing---------------------------#\n","best_score = 0.0\n","test_every, TEST = Integer(EPISODES/10), 10\n","\n","for e=1:EPISODES\n","    reset!(env)\n","    state = env.state\n","    score = 0\n","    \n","    envs = []\n","    for step=1:MAX_STEPS\n","        push!(envs, deepcopy(env))\n","        \n","        action = act(state, ϵ) # predict action\n","        reward, next_state = step!(env, state, action)\n","        done = finished(env, next_state) # check if game is finished\n","        reward = !done ? reward : -1 # penalty of -1 if game is over\n","        score += reward\n","        \n","        remember(state, action, reward, next_state, done)\n","        \n","        state = next_state\n","        done && break\n","    end\n","    \n","    stats = \"Episode: $e/$EPISODES | Score: $score | ϵ: $ϵ\"\n","    # Episode X finished after Y timesteps with Z total reward\n","    \n","    if best_score < score\n","        best_score = score\n","        println(stats); flush(stdout)\n","        @save \"models/notebook3/model-$e-$score.bson\" model\n","        anim = @animate for env in envs\n","            plot(env)\n","        end\n","        mp4(anim, \"models/notebook3/env-$e-$score.mp4\", fps=20, show_msg=false)\n","    else\n","        print(stats); flush(stdout); print(\"\\r\")\n","    end\n","    \n","    replay() # replay and learn from the episode\n","    \n","    if e % test_every == 0\n","        score = 0\n","        for i=1:TEST\n","            reset!(env)    \n","            state = env.state\n","            \n","            for step=1:MAX_STEPS\n","                action = act(state, ϵ_min)\n","                reward, state = step!(env, state, action)\n","                done = finished(env, state) # check if game is finished\n","                reward = !done ? reward : -1 # penalty of -1 if game is over\n","                score += reward\n","\n","                done && break\n","            end\n","        end\n","        \n","        score /= TEST\n","        println(\"#-- Avg Test Score $(Integer(e/test_every)) : $score --#\")\n","        score >= 200 && break\n","    end\n","end\n","\n","println(\"Done!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode: 1/500 | Score: 12.0 | ϵ: 0.9\n","Episode: 3/500 | Score: 15.0 | ϵ: 0.9\n","Episode: 4/500 | Score: 17.0 | ϵ: 0.8955\n","Episode: 6/500 | Score: 21.0 | ϵ: 0.8865673875\n","Episode: 15/500 | Score: 32.0 | ϵ: 0.8474605262229382\n","Episode: 16/500 | Score: 38.0 | ϵ: 0.8432232235918236\n","#-- Avg Test Score 1 : 17.7 --#\n","Episode: 52/500 | Score: 65.0 | ϵ: 0.7040013079012841\n","Episode: 63/500 | Score: 120.0 | ϵ: 0.6662348619270341\n","Episode: 96/500 | Score: 129.0 | ϵ: 0.564662593848428\n","#-- Avg Test Score 2 : 51.4 --#\n","Episode: 110/500 | Score: 172.0 | ϵ: 0.5263954772927323\n","#-- Avg Test Score 3 : 48.7 --#\n","#-- Avg Test Score 4 : 30.9 --#\n","#-- Avg Test Score 5 : 23.0 --#\n","#-- Avg Test Score 6 : 14.6 --#\n","#-- Avg Test Score 7 : 10.1 --#\n","#-- Avg Test Score 8 : 8.0 --#\n","#-- Avg Test Score 9 : 7.9 --#\n","#-- Avg Test Score 10 : 7.6 --#\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"nokWeBzbucbV","colab_type":"code","colab":{}},"cell_type":"code","source":["# takes about ~ 30 mins to train & test for 500 episodes and 32 batch size: giving best score of 172"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"ac68240d-2bc7-45d3-e99d-af1a1c97bc98","executionInfo":{"status":"ok","timestamp":1548542633634,"user_tz":-330,"elapsed":20589,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"id":"ZU705CuTqOT5","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["#=\n","env = CartPoleV0()\n","\n","for i=1:5\n","  plot(env)\n","  anim = Animation()\n","  for j=1:100\n","    plot!(env, title=\"env-$i-$j\")\n","    frame(anim)\n","  end\n","  mp4(anim, \"assets/notebook3/env-$i.mp4\", fps=15)\n","end\n","=#"],"execution_count":0,"outputs":[{"output_type":"stream","text":["┌ Info: Saved animation to \n","│   fn = /content/drive/My Drive/julia/assets/notebook3/env-1.mp4\n","└ @ Plots /root/.julia/packages/Plots/qh1wV/src/animation.jl:90\n","┌ Info: Saved animation to \n","│   fn = /content/drive/My Drive/julia/assets/notebook3/env-2.mp4\n","└ @ Plots /root/.julia/packages/Plots/qh1wV/src/animation.jl:90\n","┌ Info: Saved animation to \n","│   fn = /content/drive/My Drive/julia/assets/notebook3/env-3.mp4\n","└ @ Plots /root/.julia/packages/Plots/qh1wV/src/animation.jl:90\n","┌ Info: Saved animation to \n","│   fn = /content/drive/My Drive/julia/assets/notebook3/env-4.mp4\n","└ @ Plots /root/.julia/packages/Plots/qh1wV/src/animation.jl:90\n","┌ Info: Saved animation to \n","│   fn = /content/drive/My Drive/julia/assets/notebook3/env-5.mp4\n","└ @ Plots /root/.julia/packages/Plots/qh1wV/src/animation.jl:90\n"],"name":"stderr"}]},{"metadata":{"id":"PprffWiYszSj","colab_type":"code","outputId":"7d298c7d-eb53-4255-9fd3-9ce957fa7cbf","executionInfo":{"status":"ok","timestamp":1548778694053,"user_tz":-330,"elapsed":948,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# CPU-compatible function\n","# note that gpu(x) and x |> gpu do the same thing (similarly for cpu)\n","\n","\"\"\"Pick samples randomly from replay memory and train the model\"\"\"\n","function replay()\n","    length(memory) < TRAIN_START && return nothing\n","    \n","    batch_size = min(BATCH_SIZE, length(memory))\n","    minibatch = sample(memory, batch_size, replace=false)\n","    \n","    sb, ab, rb, s′b, db = collect.(zip(minibatch...))\n","    sb = hcat(sb...)\n","    s′b = hcat(s′b...)\n","    ab, rb, db = transpose.([ab, rb, db])\n","    # println(size(sb), size(ab), size(rb), size(s′b), size(db))\n","    # sb, ab, rb, s′b, db = transpose.(collect.(zip(minibatch...)))\n","    # state, action, reward, next_state, done batch (with batch dimension second)\n","    # collect(zip(ls...)) is equivalent to list(zip(*ls)) in python\n","    \n","    #=\n","    Q is a NN used as a function approximator of Q* (Bellman optimality equation)\n","    Q-learning: use any policy (model-free) to maximize future reward (just keep updating (s, a))\n","    Q[s,a] is a measure of how good an action a is for state s\n","    γ discount factor is used to value rewards received earlier (\"good start\")\n","    s -> a -> r -> s′\n","    Q[s,a] = (1 - α)Q[s,a] + α(r + γ max_{a′} Q[s′,a′])\n","    α is the learning rate\n","    r + γ max_{a′} Q[s′,a′] is the learned value (target)\n","    max_{a′} Q[s′,a′] is the estimate of future optimal value\n","    \n","    Q-learning Algorithm:\n","    initialize Q[s,a] arbitarily\n","    observe initial state s\n","    repeat\n","      select and carry out action a\n","      observe reward r and state s\n","      Q[s,a] = Q[s,a] + α(r + γ max_{a′} Q[s′,a′]  -  Q[s,a])\n","    until terminated\n","    =#\n","    \n","    qb_target = model(sb).data\n","    qb_learned = ifelse.(db, rb, rb .+ γ .* maximum(model(s′b).data, dims=1))\n","    setindex!.(Ref(qb_target), qb_learned, ab) # (1, batch_size)\n","    \n","    #=\n","    for (i, (s, a, r, s′, done)) in enumerate(minibatch)\n","        target = r + (done ? γ * maximum(model(s′ |> gpu).data) : 0)        \n","        target_f = model(s).data\n","        target_f[a] = target\n","        \n","        dataset = [(state, target_f)]\n","    end\n","    =#\n","    \n","    dataset = [(sb, qb_target)] # [(input, target)]\n","    fit_model(dataset)\n","end"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["replay"]},"metadata":{"tags":[]},"execution_count":87}]},{"metadata":{"id":"sq9d6tuAOaWI","colab_type":"code","outputId":"0bee2e89-3063-45a0-b74d-5316ac324338","executionInfo":{"status":"ok","timestamp":1548778461966,"user_tz":-330,"elapsed":1738,"user":{"displayName":"Sidhant Nagpal","photoUrl":"","userId":"00259014102912044935"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# tejank - batching for fitting dataset but not for model prediction\n","\n","\"\"\"Save sample (s, a, r, s′) to replay memory and update exploration\"\"\"\n","function remember(state, action, reward, next_state, done)\n","    push!(memory, (state, action, reward, next_state, done))\n","    \n","    global ϵ\n","    ϵ > ϵ_min && (ϵ *= ϵ_decay)\n","end\n","\n","\n","\"\"\"Get action from model using epsilon-greedy policy\"\"\"\n","function act(state)\n","    rand() <= ϵ && return rand(1:ACTION_SIZE)\n","    q_values = model(state |> gpu).data # act values\n","    return argmax(q_values)  # returns action (idx of q value)\n","end\n","\n","\n","function replay()\n","    length(memory) < TRAIN_START && return nothing\n","\n","    batch_size = min(BATCH_SIZE, length(memory))\n","    minibatch = sample(memory, batch_size, replace=false)\n","\n","    x = zeros(Float32, STATE_SIZE, batch_size)\n","    y = zeros(Float32, ACTION_SIZE, batch_size)\n","    for (i, (s, a, r, s′, done)) in enumerate(minibatch)\n","        q_learned = r\n","        !done && (q_learned += γ * maximum(model(s′ |> gpu).data))\n","\n","        q_target = model(s |> gpu).data\n","        q_target[a] = q_learned\n","\n","        x[:, i] .= s\n","        y[:, i] .= q_target\n","    end\n","\n","    dataset = [(gpu(x), gpu(y))]\n","    fit_model(dataset)\n","end"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["replay (generic function with 1 method)"]},"metadata":{"tags":[]},"execution_count":82}]}]}