{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlR1bqvJdmXN"
   },
   "outputs": [],
   "source": [
    "using DataStructures\n",
    "using BSON: @save, @load\n",
    "import Reinforce\n",
    "using Reinforce: CartPoleV0, actions, reset!, finished, step!\n",
    "using Flux, CuArrays, StatsBase, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1784,
     "status": "ok",
     "timestamp": 1548851827870,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "P8y8ZF2leZc3",
    "outputId": "c0152136-bbee-4ecf-d9c0-c44f05939c16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"100\""
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr() # gr is faster than pyplot\n",
    "ENV[\"GKSwstype\"] = \"100\" # headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2206,
     "status": "ok",
     "timestamp": 1548851828307,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "kO5PfFbM4SlA",
    "outputId": "a19a95c9-2522-4d13-f8b3-9d3d102e46c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(4, 24, NNlib.relu), Dense(24, 24, NNlib.relu), Dense(24, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------Initialize game environment----------------#\n",
    "env = CartPoleV0()\n",
    "\n",
    "\n",
    "#-------------------------Parameters-----------------------#\n",
    "EPISODES = 400\n",
    "STATE_SIZE = length(env.state)\n",
    "ACTION_SIZE = length(actions(env, env.state))\n",
    "REPLAY_MEMORY = 10000 # buffer size\n",
    "MAX_STEPS = 300 # maximum timesteps per episode\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "γ = 0.99                # discount rate\n",
    "η = 0.0001              # learning rate\n",
    "\n",
    "ϵ = 1.0                 # exploration rate\n",
    "ϵ_min = 0.01            # exploration minimum\n",
    "ϵ_decay = 0.99          # exploration decay\n",
    "\n",
    "memory = CircularBuffer{Any}(REPLAY_MEMORY)\n",
    "\n",
    "\n",
    "#-----------------------Model Architecture------------------------#\n",
    "model = Chain(Dense(STATE_SIZE, 24, relu),\n",
    "              Dense(24, 24, relu), \n",
    "              Dense(24, ACTION_SIZE)) |> gpu\n",
    "\n",
    "loss(x, y) = Flux.mse(model(x), y)\n",
    "opt = ADAM(η)\n",
    "\n",
    "fit_model(dataset) = Flux.train!(loss, params(model), dataset, opt)\n",
    "\n",
    "model_target = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1548851828310,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "GO-ZLvfDe9K0",
    "outputId": "fafaee64-1908-41a6-c7af-bec5140d05c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "replay (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function remember(state, action, reward, next_state, done)\n",
    "    push!(memory, (state, action, reward, next_state, done))\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"Decaying Epsilon-Greedy Policy for selecting action (accept random actions with ϵ probability)\"\"\"\n",
    "function act(state, ϵ)\n",
    "    rand() <= ϵ && return rand(1:ACTION_SIZE) # explore\n",
    "    q_values = model(state |> gpu).data # act values\n",
    "    return argmax(q_values)  # returns action (idx of q value)\n",
    "end\n",
    "\n",
    "\n",
    "function replay()\n",
    "    length(memory) < BATCH_SIZE && return nothing\n",
    "    \n",
    "    batch_size = min(BATCH_SIZE, length(memory))\n",
    "    minibatch = sample(memory, batch_size, replace=false)\n",
    "    \n",
    "    sb, ab, rb, s′b, db = collect.(zip(minibatch...))\n",
    "    sb = hcat(sb...) |> gpu\n",
    "    s′b = hcat(s′b...) |> gpu\n",
    "    \n",
    "    qb_target = model(sb).data\n",
    "    \n",
    "    actions = argmax(model(s′b).data) # 1 addition in Double DQN: action using model\n",
    "    qb_learn = model_target(s′b).data # 1 change in Double DQN: update using target model\n",
    "    qb_learned = getindex.(Ref(qb_learn), actions)\n",
    "    \n",
    "    qb_learned = ifelse.(db, rb, rb .+ γ .* cpu(qb_learned))\n",
    "    setindex!.(Ref(qb_target), qb_learned, ab) # (1, batch_size)\n",
    "    \n",
    "    dataset = [(sb, qb_target)] # [(input, target)]\n",
    "    fit_model(dataset)\n",
    "    \n",
    "    GC.gc(); # CuArrays.clearpool()\n",
    "    \n",
    "    global ϵ\n",
    "    ϵ = ϵ_min + (ϵ - ϵ_min)*ϵ_decay\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1148658,
     "status": "ok",
     "timestamp": 1548852974788,
     "user": {
      "displayName": "Sidhant Nagpal",
      "photoUrl": "",
      "userId": "00259014102912044935"
     },
     "user_tz": -330
    },
    "id": "-GrymTYc17Q0",
    "outputId": "06a2e880-05c7-406c-eb57-d6d3c2cf469b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400 | Score: 22.0 | ϵ: 1.0\n",
      "Episode: 5/400 | Score: 39.0 | ϵ: 0.97059601\n",
      "Episode: 23/400 | Score: 48.0 | ϵ: 0.8116305895390458\n",
      "Episode: 35/400 | Score: 81.0 | ϵ: 0.7205532272722921\n",
      "#-- Avg Test Score 1 : 12.4 --#\n",
      "#-- Avg Test Score 2 : 21.7 --#\n",
      "#-- Avg Test Score 3 : 26.8 --#\n",
      "#-- Avg Test Score 4 : 27.4 --#\n",
      "#-- Avg Test Score 5 : 29.7 --#\n",
      "#-- Avg Test Score 6 : 31.2 --#\n",
      "#-- Avg Test Score 7 : 39.4 --#\n",
      "#-- Avg Test Score 8 : 38.5 --#\n",
      "Episode: 337/400 | Score: 81.0 | ϵ: 0.0441527268562123\n",
      "Episode: 338/400 | Score: 96.0 | ϵ: 0.043811199587650174\n",
      "#-- Avg Test Score 9 : 38.8 --#\n",
      "Episode: 398/400 | Score: 125.0 | ϵ: 0.02850002244158248\n",
      "#-- Avg Test Score 10 : 42.2 --#\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Training & Testing---------------------------#\n",
    "best_score = 0.0\n",
    "test_every, TEST = Integer(EPISODES/10), 10\n",
    "\n",
    "for e=1:EPISODES\n",
    "    reset!(env)\n",
    "    state = env.state\n",
    "    score = 0\n",
    "    \n",
    "    envs = []\n",
    "    for step=1:MAX_STEPS\n",
    "        push!(envs, deepcopy(env))\n",
    "        \n",
    "        action = act(state, ϵ) # predict action\n",
    "        reward, next_state = step!(env, state, action)\n",
    "        done = finished(env, next_state) # check if game is finished\n",
    "        reward = !done ? reward : -1 # penalty of -1 if game is over\n",
    "        score += reward\n",
    "        \n",
    "        remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        done && break\n",
    "    end\n",
    "    \n",
    "    stats = \"Episode: $e/$EPISODES | Score: $score | ϵ: $ϵ\"\n",
    "    # Episode X finished after Y timesteps with Z total reward\n",
    "    \n",
    "    if best_score <= score\n",
    "        best_score = score\n",
    "        println(stats); flush(stdout)\n",
    "        @save \"models/notebook4/model-$e-$score.bson\" model\n",
    "        anim = @animate for env in envs\n",
    "            plot(env)\n",
    "        end\n",
    "        mp4(anim, \"models/notebook4/env-$e-$score.mp4\", fps=20, show_msg=false)\n",
    "    else\n",
    "        print(stats); flush(stdout); print(\"\\r\")\n",
    "    end\n",
    "    \n",
    "    replay() # replay and learn from the episode\n",
    "    model_target = deepcopy(model) # after each episode make target model same as model\n",
    "    \n",
    "    if e % test_every == 0\n",
    "        score = 0\n",
    "        for i=1:TEST\n",
    "            reset!(env)    \n",
    "            state = env.state\n",
    "            \n",
    "            for step=1:MAX_STEPS\n",
    "                action = act(state, ϵ_min)\n",
    "                reward, state = step!(env, state, action)\n",
    "                done = finished(env, state) # check if game is finished\n",
    "                reward = !done ? reward : -1 # penalty of -1 if game is over\n",
    "                score += reward\n",
    "\n",
    "                done && break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        score /= TEST\n",
    "        println(\"#-- Avg Test Score $(Integer(e/test_every)) : $score --#\")\n",
    "        score >= 200 && break\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "04 - Flux Double DQN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.1",
   "name": "julia-1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
